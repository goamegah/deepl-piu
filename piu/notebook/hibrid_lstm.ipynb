{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import argparse\n",
    "from piu.models.rnn import LSTMWithTabular\n",
    "from piu.data.lstm_dataset import MixedDataSequenceDataset, get_dataloaders\n",
    "from piu.utils.lstm_utils import train, evaluate\n",
    "from definitions import SERIES_TRAIN_DATA_PATH, TRAIN_DATA_PATH\n",
    "\n",
    "\n",
    "def get_optimizer(model, args):\n",
    "    if args.optimizer == \"adam\":\n",
    "        return optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    elif args.optimizer == \"sgd\":\n",
    "        return optim.SGD(model.parameters(), lr=args.learning_rate, momentum=0.9)\n",
    "    elif args.optimizer == \"rmsprop\":\n",
    "        return optim.RMSprop(model.parameters(), lr=args.learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Optimiseur non reconnu.\")\n",
    "\n",
    "def get_scheduler(optimizer, args):\n",
    "    if args.scheduler == \"step\":\n",
    "        return optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    elif args.scheduler == \"cosine\":\n",
    "        return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.num_epochs)\n",
    "    elif args.scheduler == \"none\":\n",
    "        return None\n",
    "\n",
    "def inference(model, parquet_dir, csv_path, inference_id, device):\n",
    "    print(f\" Inf√©rence pour l'ID {inference_id}\")\n",
    "\n",
    "    # Pr√©paration des donn√©es\n",
    "    X_seq, X_static, id_ = MixedDataSequenceDataset.prepare_sample_for_inference(parquet_dir, csv_path, inference_id)\n",
    "\n",
    "    X_seq = X_seq.to(device).unsqueeze(0)\n",
    "    X_static = X_static.to(device).unsqueeze(0)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(X_seq, X_static)\n",
    "        prediction = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    print(f\"üîÆ Pr√©diction pour l'ID {id_}: {prediction}\")\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# üîπ Fonction principale\n",
    "def main():\n",
    "    args = get_args()\n",
    "    wandb.init(\n",
    "    project=\"Hybrid_LSTM_Tabulardata\",\n",
    "    config=vars(args),\n",
    "    name=f\"Hybrid_LSTM_{args.mode}_\"\n",
    "         f\"fts={args.feature_selection}_\"\n",
    "         f\"imb={args.imbalance_handling}_\"\n",
    "         f\"da={args.data_augmentation}_\"\n",
    "         f\"opt={args.optimizer}_\"\n",
    "         f\"sch={args.scheduler}_\"\n",
    "         f\"metric={args.metric}\".replace(\" \", \"\")\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Chargement des donn√©es\n",
    "    train_parquet_dir = SERIES_TRAIN_DATA_PATH\n",
    "    train_csv_path = TRAIN_DATA_PATH\n",
    "\n",
    "    if args.mode in [\"train\", \"test\"]:\n",
    "        train_loader, val_loader = get_dataloaders(\n",
    "            train_parquet_dir, train_csv_path, batch_size=args.batch_size, split=\"both\", feature_selection=args.feature_selection\n",
    "        )\n",
    "\n",
    "        # D√©terminer les dimensions d'entr√©e dynamiquement\n",
    "        sample_X_seq, sample_X_static, _ = next(iter(train_loader))\n",
    "        input_dim_seq = sample_X_seq.shape[-1]\n",
    "        input_dim_static = sample_X_static.shape[-1]\n",
    "        output_dim = len(torch.unique(torch.tensor([train_loader.dataset[i][2] for i in range(len(train_loader.dataset))])))\n",
    "\n",
    "        # Initialisation du mod√®le\n",
    "        model = LSTMWithTabular(\n",
    "            input_dim_seq=input_dim_seq,\n",
    "            hidden_dim=args.hidden_dim,\n",
    "            num_layers=args.num_layers,\n",
    "            input_dim_static=input_dim_static,\n",
    "            output_dim=output_dim\n",
    "        ).to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = get_optimizer(model, args)\n",
    "        scheduler = get_scheduler(optimizer, args)\n",
    "\n",
    "        # Suivi WandB\n",
    "        wandb.watch(model, log=\"all\")\n",
    "\n",
    "        # Entra√Ænement\n",
    "        if args.mode == \"train\":\n",
    "            train(model, train_loader, val_loader, criterion, optimizer, scheduler, device, args)\n",
    "\n",
    "            # Sauvegarde du mod√®le\n",
    "            torch.save(model.state_dict(), args.save_path)\n",
    "            print(f\" Mod√®le sauvegard√© sous {args.save_path}\")\n",
    "\n",
    "        # √âvaluation\n",
    "        elif args.mode == \"test\":\n",
    "            if not os.path.exists(args.save_path):\n",
    "                raise ValueError(f\"‚ùå Mod√®le introuvable √† {args.save_path}, assurez-vous de l'avoir entra√Æn√©.\")\n",
    "\n",
    "            model.load_state_dict(torch.load(args.save_path, map_location=device))\n",
    "            evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    # Mode inf√©rence\n",
    "    elif args.mode == \"inference\":\n",
    "        if args.inference_id is None:\n",
    "            raise ValueError(\"‚ùå L'ID pour l'inf√©rence doit √™tre sp√©cifi√© avec --inference_id\")\n",
    "\n",
    "        if not os.path.exists(args.save_path):\n",
    "            raise ValueError(f\"‚ùå Mod√®le introuvable √† {args.save_path}, assurez-vous de l'avoir entra√Æn√©.\")\n",
    "\n",
    "        # Chargement du mod√®le\n",
    "        model = LSTMWithTabular(\n",
    "            input_dim_seq=input_dim_seq,\n",
    "            hidden_dim=args.hidden_dim,\n",
    "            num_layers=args.num_layers,\n",
    "            input_dim_static=input_dim_static,\n",
    "            output_dim=output_dim\n",
    "        ).to(device)\n",
    "\n",
    "        model.load_state_dict(torch.load(args.save_path, map_location=device))\n",
    "        inference(model, train_parquet_dir, train_csv_path, args.inference_id, device)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
